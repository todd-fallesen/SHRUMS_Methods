{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61d600b",
   "metadata": {},
   "source": [
    "# SHRUMS Data Wrangling 5 channels\n",
    "\n",
    "## Jupyter notebook implementation\n",
    "\n",
    "This is an implementation of the SHRUMS data wrangling in python. As CellProfiler processes the images, it will detect the nuclei in each Z slice, and then combine them using tracking.  The function of this script is to integrate the the data from each 3D nucleus across images in a Z-stack, using their tracking ID number.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82092aee",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62825470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a4ec8e",
   "metadata": {},
   "source": [
    "## Set the input paths, the filename and save_file_name\n",
    "\n",
    "The **input_path** is the top level directory where all processed subdirectories are\n",
    "\n",
    "The **filename** is the filename in each directory that contains the data from that image set \n",
    "\n",
    "The **save_file_name** is what the output file will be named\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##CHANGE THESE\n",
    "#Path to directory\n",
    "input_path = '/path/to/default/Output_Folder/' #the directory where all the output subdirectories are\n",
    "filename='Stardist_Labels_Enhanced_ImagesObjects_edge_size_filtered.csv'  #the file we will be looking for in each subdirectory\n",
    "save_file_name  = 'Date_Meaningful_Filename_All_Channels_Output.xlsx'  #this is the output file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7378fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get file list\n",
    "# Concatenate input_path and save_file_name\n",
    "all_data_output_file = os.path.join(input_path, save_file_name)\n",
    "save_path = input_path\n",
    "\n",
    "#get a list of all the files and subdirectories in the main folder\n",
    "all_files = os.listdir(input_path)\n",
    "\n",
    "# get a list of all the subdirectories only\n",
    "sub_folders = [f for f in all_files if os.path.isdir(os.path.join(input_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are: \", len(sub_folders), \"folders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a7e5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n=1\n",
    "for folder in sub_folders:\n",
    "    print(\"run number\", n)\n",
    "    \n",
    "    id = folder\n",
    "    id_short = id[:31] if len(id) > 31 else id\n",
    "    print(\"id_short: \", id_short)\n",
    "    \n",
    "    csv_input_file = os.path.join(input_path, id, filename)\n",
    "    excel_out_file = f\"{id}.xlsx\"\n",
    "    SaveFileName = os.path.join(save_path, excel_out_file)\n",
    "    print(\"input file: \", csv_input_file)\n",
    "    print(\"save_file_path: \", SaveFileName)\n",
    "    \n",
    "    ##read in the file, sometimes if coming from the server, this fails the first time, so we retry after a second\n",
    "        \n",
    "    try:\n",
    "        filename_table = pd.read_csv(csv_input_file)\n",
    "        \n",
    "    except Exception:\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            filename_table = pd.read_csv(csv_input_file)\n",
    "        except Exception: #check to make sure the folder had the right file\n",
    "            print(\"Folder: \", csv_input_file, \" didn't work\")\n",
    "            continue\n",
    "    if filename_table.empty:\n",
    "        time.sleep(1)\n",
    "        print(\"Stuck on File\")\n",
    "        print(csv_input_file)\n",
    "        filename_table = pd.read_csv(csv_input_file)\n",
    "        \n",
    "    if not filename_table.empty:\n",
    "    # Select specific columns for Small_Table\n",
    "        cols = [\n",
    "            'ImageNumber', \n",
    "            'ObjectNumber',\n",
    "            'Intensity_IntegratedIntensity_C1',\n",
    "            'Intensity_IntegratedIntensity_C1_Corr',\n",
    "            'Intensity_IntegratedIntensity_C1_Enhanced',\n",
    "            'Intensity_IntegratedIntensity_C1_Enhanced_Corr',\n",
    "            'Intensity_IntegratedIntensity_C2',\n",
    "            'Intensity_IntegratedIntensity_C2_Corr',\n",
    "            'Intensity_IntegratedIntensity_C2_Enhanced_Corr',\n",
    "            'Intensity_IntegratedIntensity_C2_enhanced',\n",
    "            'Intensity_IntegratedIntensity_C3',\n",
    "            'Intensity_IntegratedIntensity_C3_Corr',\n",
    "            'Intensity_IntegratedIntensity_C3_Enhanced',\n",
    "            'Intensity_IntegratedIntensity_C3_Enhanced_Corr',\n",
    "            'Intensity_IntegratedIntensity_C4',\n",
    "            'Intensity_IntegratedIntensity_C4_Corr',\n",
    "            'Intensity_IntegratedIntensity_C4_Enhanced',\n",
    "            'Intensity_IntegratedIntensity_C4_Enhanced_Corr',\n",
    "            'Intensity_IntegratedIntensity_C5',\n",
    "            'AreaShape_Eccentricity', \n",
    "            'AreaShape_Area',\n",
    "            'TrackObjects_Label_2',\n",
    "            'TrackObjects_Lifetime_2'\n",
    "        ]\n",
    "    small_table = filename_table[cols]\n",
    "    tracked_objects = small_table['TrackObjects_Label_2'].dropna().unique()\n",
    "\n",
    "    total_intensity = []\n",
    "    ## the tallying section\n",
    "    \n",
    "    for obj_id in tracked_objects:\n",
    "        obj_data = small_table[small_table['TrackObjects_Label_2'] == obj_id]\n",
    "\n",
    "        total_intensity.append([\n",
    "            obj_id,\n",
    "            obj_data['Intensity_IntegratedIntensity_C1'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C1_Corr'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C1_Enhanced'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C1_Enhanced_Corr'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C2'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C2_Corr'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C2_Enhanced_Corr'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C2_enhanced'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C3'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C3_Corr'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C3_Enhanced'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C3_Enhanced_Corr'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C4'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C4_Corr'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C4_Enhanced'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C4_Enhanced_Corr'].sum(),\n",
    "            obj_data['Intensity_IntegratedIntensity_C5'].sum(),\n",
    "            obj_data['AreaShape_Eccentricity'].max(),\n",
    "            obj_data['AreaShape_Area'].sum(),\n",
    "            obj_data['TrackObjects_Lifetime_2'].max()\n",
    "        ])\n",
    "\n",
    "    column_headers = [\n",
    "        'Tracked_Object_Number',\n",
    "        'Intensity_IntegratedIntensity_C1',\n",
    "        'Intensity_IntegratedIntensity_C1_Corr',\n",
    "        'Intensity_IntegratedIntensity_C1_Enhanced',\n",
    "        'Intensity_IntegratedIntensity_C1_Enhanced_Corr',\n",
    "        'Intensity_IntegratedIntensity_C2',\n",
    "        'Intensity_IntegratedIntensity_C2_Corr',\n",
    "        'Intensity_IntegratedIntensity_C2_Enhanced_Corr',\n",
    "        'Intensity_IntegratedIntensity_C2_enhanced',\n",
    "        'Intensity_IntegratedIntensity_C3',\n",
    "        'Intensity_IntegratedIntensity_C3_Corr',\n",
    "        'Intensity_IntegratedIntensity_C3_Enhanced',\n",
    "        'Intensity_IntegratedIntensity_C3_Enhanced_Corr',\n",
    "        'Intensity_IntegratedIntensity_C4',\n",
    "        'Intensity_IntegratedIntensity_C4_Corr',\n",
    "        'Intensity_IntegratedIntensity_C4_Enhanced',\n",
    "        'Intensity_IntegratedIntensity_C4_Enhanced_Corr',\n",
    "        'Intensity_IntegratedIntensity_C5',\n",
    "        'Max_Eccentricity',\n",
    "        'Total_Area',\n",
    "        'Lifetime'\n",
    "    ]\n",
    "\n",
    "    output_df = pd.DataFrame(total_intensity, columns=column_headers)\n",
    "    time.sleep(2)\n",
    "    output_df.to_excel(SaveFileName, index=False)\n",
    "    print(SaveFileName)\n",
    "    time.sleep(5)\n",
    "    print(\"n is: \", n)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if n == 1:  ##on the first iteration, create the excel file\n",
    "        mode = 'w'\n",
    "        with pd.ExcelWriter(all_data_output_file, mode=mode, engine='openpyxl') as writer:\n",
    "            output_df.to_excel(writer, sheet_name=id_short, index=False)\n",
    "   \n",
    "    else:  ##append a new sheet to the excel file\n",
    "        mode = 'a'\n",
    "        with pd.ExcelWriter(all_data_output_file, mode=mode, engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "            output_df.to_excel(writer, sheet_name=id_short, index=False)\n",
    "    n=n+1 #iterate for the ID number.  This is done at the end, so that files that fail, and get bumped by the exception block don't iterate the excel file sheets, \n",
    "          # or in worse case scenario, if the first folder read fails, then this will allow us to still write the excel out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b2bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local sparse",
   "language": "",
   "name": "rik_local_sparse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
